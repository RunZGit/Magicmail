{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic(srcaddr,indaddr,topN = 10, N = 9):\n",
    "    \n",
    "    src = pd.read_csv(srcaddr)\n",
    "    ind = pd.read_csv(indaddr)\n",
    "    \n",
    "    ar = []\n",
    "    for i in range(N):\n",
    "        ar.append([])\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    irrelevant_lists = ['rose','Rose','I','edu','hulman','Hulman','www','com','1','2','3','4','5','6','7','8','9','0','If','It','it','if']\n",
    "    for i in range(src.shape[0]):\n",
    "        index = int(ind.cluster[i])\n",
    "        string = str(src.Title[i])+\" \"+str(src.Body[i])\n",
    "        a = tokenizer.tokenize(string)\n",
    "        filtered_word = [word for word in a if word not in stopwords.words('english')]\n",
    "        for word in filtered_word:\n",
    "            if(word not in irrelevant_lists):\n",
    "                ar[index].append(word)\n",
    "    \n",
    "    top10 = []\n",
    "    for k in range(N):\n",
    "        maps={}\n",
    "        for word in ar[k]:\n",
    "            maps[word]=0\n",
    "        for word in ar[k]:\n",
    "            maps[word] = maps[word]+1\n",
    "        temp = []\n",
    "        for h in range(topN):\n",
    "            mapscount = 0\n",
    "            mapskey = \"\"\n",
    "            for key in maps:\n",
    "                if(maps[key]>mapscount):\n",
    "                    mapskey = key\n",
    "                    mapscount = maps[key]\n",
    "            maps[mapskey]=0\n",
    "            temp.append(mapskey)\n",
    "        top10.append(temp)\n",
    "    return top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Selling', 'selling', 'sale', 'interested', 'condition', 'asking', 'new', 'email', 'used', 'http']\n",
      "['interested', 'room', 'looking', 'email', 'loft', 'would', 'year', 'Percopo', 'We', 'Thanks']\n",
      "['book', 'Selling', 'selling', 'books', 'interested', 'Book', 'edition', 'email', 'used', 'Books']\n",
      "['font', '0in', 'serif', 'family', 'sans', 'span', 'color', 'MsoNormal', 'Calibri', 'margin']\n",
      "['Lost', 'please', 'lost', 'found', 'email', 'black', 'Thanks', 'Thank', 'contact', 'know']\n",
      "['ride', 'Indy', 'gas', 'pay', 'Ride', 'airport', 'need', 'back', 'flight', 'Airport']\n",
      "['The', 'Please', 'SimplyWell', 'please', 'IM', 'register', '00', 'points', '2014', 'Wednesday']\n",
      "['found', 'claim', 'A', 'Found', 'describe', 'phone', 'Please', 'cell', 'Moench', 'please']\n",
      "['The', 'A', 'interested', 'http', '10', 'please', 'contact', 'email', 'Selling', 'one']\n"
     ]
    }
   ],
   "source": [
    "result = topic('announcements_cbow.csv','tensorflow.csv')\n",
    "for words in result:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
