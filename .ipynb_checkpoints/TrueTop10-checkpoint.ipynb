{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function can find the top N common words of every cluster.\n",
    "def topic(srcaddr,indaddr,topN = 10, N = 9):\n",
    "    # Import packages\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    \n",
    "    # Load the original csv file and the vectorized csv file\n",
    "    src = pd.read_csv(srcaddr)\n",
    "    ind = pd.read_csv(indaddr)\n",
    "    \n",
    "    # Create an 2D array to store the words of the original csv file according to the cluster number of vectorized csv file\n",
    "    ar = []\n",
    "    for i in range(N):\n",
    "        ar.append([])\n",
    "    \n",
    "    # Initialize the punctuation eliminator\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Create a list of irrelevant words\n",
    "    irrelevant_lists = ['rose','Rose','I','edu','hulman','Hulman','www','com','http','The','1','2','3','4','5','6','7','8','9','0','If','It','it','if']\n",
    "    # Loop through all rows\n",
    "    for i in range(src.shape[0]):\n",
    "        # Add the title and the body in the same row to a string\n",
    "        index = int(ind.cluster[i])\n",
    "        string = str(src.Title[i])+\" \"+str(src.Body[i])\n",
    "        # Get rid of the punctuation, stopwords and the irrelevant words\n",
    "        a = tokenizer.tokenize(string)\n",
    "        filtered_word = [word for word in a if word not in stopwords.words('english')]\n",
    "        for word in filtered_word:\n",
    "            if(word not in irrelevant_lists):\n",
    "                # Append the word list to ar of index which we get from cluster number of vectorized csv\n",
    "                ar[index].append(word)\n",
    "    \n",
    "    # Create an empty array to store top N words of every cluster\n",
    "    top10 = []\n",
    "    # Loop through number of clusters.\n",
    "    for k in range(N):\n",
    "        # Create an empty map\n",
    "        maps={}\n",
    "        # Store the number of a word as value and the word as key to the map\n",
    "        for word in ar[k]:\n",
    "            maps[word]=0\n",
    "        for word in ar[k]:\n",
    "            maps[word] = maps[word]+1\n",
    "        # Create an empty array to store top N words of a cluster\n",
    "        temp = []\n",
    "        # Loop through to find the top N words of a cluster\n",
    "        for h in range(topN):\n",
    "            mapscount = 0\n",
    "            mapskey = \"\"\n",
    "            for key in maps:\n",
    "                if(maps[key]>mapscount):\n",
    "                    mapskey = key\n",
    "                    mapscount = maps[key]\n",
    "            maps[mapskey]=0\n",
    "            temp.append(mapskey)\n",
    "        # Store the temp array to top10 array\n",
    "        top10.append(temp)\n",
    "    # return top10 2Darray\n",
    "    return top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Selling', 'selling', 'sale', 'interested', 'condition', 'asking', 'new', 'email', 'used', 'Sale']\n",
      "['interested', 'room', 'looking', 'email', 'loft', 'would', 'year', 'Percopo', 'We', 'Thanks']\n",
      "['book', 'Selling', 'selling', 'books', 'interested', 'Book', 'edition', 'email', 'used', 'Books']\n",
      "['font', '0in', 'family', 'serif', 'sans', 'span', 'color', 'MsoNormal', 'Calibri', 'margin']\n",
      "['Lost', 'please', 'lost', 'found', 'email', 'black', 'Thanks', 'Thank', 'contact', 'know']\n",
      "['ride', 'Indy', 'gas', 'pay', 'Ride', 'airport', 'need', 'back', 'flight', 'Airport']\n",
      "['Please', 'SimplyWell', 'please', 'IM', 'register', '00', 'points', '2014', 'Wednesday', 'Intramural']\n",
      "['found', 'claim', 'A', 'Found', 'describe', 'phone', 'Please', 'cell', 'Moench', 'please']\n",
      "['interested', 'A', '10', 'contact', 'please', 'email', 'Selling', 'one', '30', 'new']\n"
     ]
    }
   ],
   "source": [
    "result = topic('announcements_cbow.csv','tensorflow.csv')\n",
    "for words in result:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
